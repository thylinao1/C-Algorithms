# C++ Algorithm Implementations from Scratch

Three production-quality implementations of fundamental machine learning and optimization algorithms, built from first principles in C++ with comprehensive mathematical documentation.

---

## ğŸ“¦ Contents

1. [Neural Network with Backpropagation](#1-neural-network-with-backpropagation)
2. [Gaussian Distribution Fitting](#2-gaussian-distribution-fitting)
3. [PageRank Algorithm](#3-pagerank-algorithm)

---

## 1. Neural Network with Backpropagation

### Overview
Feed-forward neural network with 3 layers implementing manual backpropagation via chain rule. No ML libraries - pure implementation using only linear algebra.

### Architecture
```
Input Layer (1) â†’ Hidden Layer 1 (6) â†’ Hidden Layer 2 (7) â†’ Output Layer (2)
```

### Mathematical Foundation

#### Forward Propagation

**Layer 1:**
```
zâ‚ = Wâ‚x + bâ‚
aâ‚ = Ïƒ(zâ‚)
```

**Layer 2:**
```
zâ‚‚ = Wâ‚‚aâ‚ + bâ‚‚
aâ‚‚ = Ïƒ(zâ‚‚)
```

**Layer 3 (Output):**
```
zâ‚ƒ = Wâ‚ƒaâ‚‚ + bâ‚ƒ
aâ‚ƒ = Ïƒ(zâ‚ƒ)
```

#### Activation Function

**Sigmoid:**
```
Ïƒ(z) = 1 / (1 + eâ»á¶»)
```

**Derivative:**
```
Ïƒ'(z) = cosh(z/2)â»Â² / 4
```

#### Loss Function

**Mean Squared Error:**
```
C = 1/n Î£áµ¢ ||aáµ¢â½Â³â¾ - yáµ¢||Â²
```

#### Backpropagation Gradients

**Output Layer (Layer 3):**
```
âˆ‚C/âˆ‚Wâ‚ƒ = (1/n) Â· âˆ‚C/âˆ‚aâ‚ƒ âŠ™ Ïƒ'(zâ‚ƒ) Â· aâ‚‚áµ€
âˆ‚C/âˆ‚bâ‚ƒ = (1/n) Â· Î£â±¼[âˆ‚C/âˆ‚aâ‚ƒ âŠ™ Ïƒ'(zâ‚ƒ)]

where: âˆ‚C/âˆ‚aâ‚ƒ = 2(aâ‚ƒ - y)
```

**Hidden Layer 2:**
```
âˆ‚C/âˆ‚Wâ‚‚ = (1/n) Â· [Wâ‚ƒáµ€(âˆ‚C/âˆ‚aâ‚ƒ âŠ™ Ïƒ'(zâ‚ƒ))] âŠ™ Ïƒ'(zâ‚‚) Â· aâ‚áµ€
âˆ‚C/âˆ‚bâ‚‚ = (1/n) Â· Î£â±¼[Wâ‚ƒáµ€(âˆ‚C/âˆ‚aâ‚ƒ âŠ™ Ïƒ'(zâ‚ƒ)) âŠ™ Ïƒ'(zâ‚‚)]
```

**Hidden Layer 1:**
```
âˆ‚C/âˆ‚Wâ‚ = (1/n) Â· [Wâ‚‚áµ€(Wâ‚ƒáµ€(âˆ‚C/âˆ‚aâ‚ƒ âŠ™ Ïƒ'(zâ‚ƒ)) âŠ™ Ïƒ'(zâ‚‚))] âŠ™ Ïƒ'(zâ‚) Â· xáµ€
âˆ‚C/âˆ‚bâ‚ = (1/n) Â· Î£â±¼[Wâ‚‚áµ€(Wâ‚ƒáµ€(âˆ‚C/âˆ‚aâ‚ƒ âŠ™ Ïƒ'(zâ‚ƒ)) âŠ™ Ïƒ'(zâ‚‚)) âŠ™ Ïƒ'(zâ‚)]
```

**Notation:**
- `âŠ™` = element-wise (Hadamard) product
- `Â·` = matrix multiplication
- `áµ€` = transpose

#### Gradient Descent Update

```
W â† W - Î± Â· âˆ‚C/âˆ‚W
b â† b - Î± Â· âˆ‚C/âˆ‚b
```

where Î± is the learning rate.

### Key Concepts
- **Chain Rule:** Derivatives flow backward through layers
- **Jacobian Matrices:** Gradients for each parameter
- **Batch Processing:** Efficient matrix operations for multiple samples
- **Weight Initialization:** Small random values prevent symmetry

### Complexity
- **Forward pass:** O(nÂ² Â· m) where n = layer size, m = samples
- **Backward pass:** O(nÂ² Â· m)
- **Per epoch:** O(nÂ² Â· m Â· k) where k = number of layers

---

## 2. Gaussian Distribution Fitting

### Overview
Fits a Gaussian (normal) distribution to empirical data by minimizing chi-squared error using steepest descent with analytical gradients.

### Problem Statement
Given data points (xáµ¢, yáµ¢), find optimal parameters Î¼ (mean) and Ïƒ (standard deviation) that best fit:

```
f(x; Î¼, Ïƒ) = (1/âˆš(2Ï€ÏƒÂ²)) Â· exp(-(x-Î¼)Â²/(2ÏƒÂ²))
```

### Mathematical Foundation

#### Gaussian PDF

```
f(x; Î¼, Ïƒ) = 1/(Ïƒâˆš(2Ï€)) Â· exp(-(x-Î¼)Â²/(2ÏƒÂ²))
```

**Parameters:**
- Î¼: Mean (center of distribution)
- Ïƒ: Standard deviation (width of distribution)
- ÏƒÂ²: Variance

#### Cost Function

**Chi-Squared Error:**
```
Ï‡Â² = Î£áµ¢ (yáµ¢ - f(xáµ¢; Î¼, Ïƒ))Â²
```

Goal: Minimize Ï‡Â² with respect to Î¼ and Ïƒ

#### Analytical Gradients

**Partial Derivative w.r.t. Î¼:**

Starting from:
```
f(x) = (1/âˆš(2Ï€ÏƒÂ²)) Â· exp(-(x-Î¼)Â²/(2ÏƒÂ²))
```

Using chain rule:
```
âˆ‚f/âˆ‚Î¼ = f(x) Â· âˆ‚/âˆ‚Î¼[-(x-Î¼)Â²/(2ÏƒÂ²)]
      = f(x) Â· [2(x-Î¼)/(2ÏƒÂ²)]
      = f(x) Â· (x-Î¼)/ÏƒÂ²
```

**Partial Derivative w.r.t. Ïƒ:**

Rewrite f as:
```
f(x) = (2Ï€)â»Â¹/Â² Â· Ïƒâ»Â¹ Â· exp(-(x-Î¼)Â²/(2ÏƒÂ²))
```

Using product rule and chain rule:
```
âˆ‚f/âˆ‚Ïƒ = (2Ï€)â»Â¹/Â² Â· [(-Ïƒâ»Â²) Â· exp(...) + Ïƒâ»Â¹ Â· exp(...) Â· (x-Î¼)Â²/ÏƒÂ³]
      = f(x) Â· [-1/Ïƒ + (x-Î¼)Â²/ÏƒÂ³]
```

#### Gradient of Chi-Squared

```
âˆ‚Ï‡Â²/âˆ‚Î¼ = Î£áµ¢ âˆ‚/âˆ‚Î¼[(yáµ¢ - f(xáµ¢))Â²]
       = -2 Î£áµ¢ (yáµ¢ - f(xáµ¢)) Â· âˆ‚f/âˆ‚Î¼

âˆ‚Ï‡Â²/âˆ‚Ïƒ = -2 Î£áµ¢ (yáµ¢ - f(xáµ¢)) Â· âˆ‚f/âˆ‚Ïƒ
```

#### Steepest Descent Update

```
Î¼â½áµ—âºÂ¹â¾ = Î¼â½áµ—â¾ - Î± Â· âˆ‚Ï‡Â²/âˆ‚Î¼
Ïƒâ½áµ—âºÂ¹â¾ = Ïƒâ½áµ—â¾ - Î± Â· âˆ‚Ï‡Â²/âˆ‚Ïƒ
```

where Î± is the learning rate.

### Convergence Analysis

**Necessary Conditions (Karush-Kuhn-Tucker):**
```
âˆ‚Ï‡Â²/âˆ‚Î¼ = 0
âˆ‚Ï‡Â²/âˆ‚Ïƒ = 0
Ïƒ > 0 (constraint)
```

**Convergence Rate:**
- Linear convergence: ||Î¸â½áµ—âºÂ¹â¾ - Î¸*|| â‰¤ c||Î¸â½áµ—â¾ - Î¸*|| for some c < 1
- Typical iterations to convergence: 20-50 for well-conditioned problems

### Complexity
- **Per iteration:** O(n) where n = number of data points
- **Total:** O(kÂ·n) where k = iterations to convergence

---

## 3. PageRank Algorithm

### Overview
Computes node importance in networks using Markov chain theory. Two implementations: power iteration (O(knÂ²)) and eigendecomposition (O(nÂ³)).

### Problem Statement
Given a network represented by link matrix L, find the stationary distribution r that satisfies:

```
r = Mr
```

where M is the Google matrix.

### Mathematical Foundation

#### Link Matrix L

Column-stochastic matrix where:
```
L[i,j] = probability of transition FROM node j TO node i

Properties:
Î£áµ¢ L[i,j] = 1  (each column sums to 1)
L[i,j] â‰¥ 0      (non-negative entries)
```

#### Google Matrix

```
M = dL + ((1-d)/n)J
```

**Components:**
- d: Damping factor (typically 0.85)
- L: Link matrix
- J: Matrix of all ones (nÃ—n)
- n: Number of nodes

**Interpretation:**
- With probability d (85%): Follow random outgoing link
- With probability 1-d (15%): Jump to random page (teleportation)

#### PageRank Equation

```
r = Mr
```

This is an **eigenvector equation** with eigenvalue Î» = 1.

#### Perron-Frobenius Theorem

For primitive, non-negative matrix M:

1. **Existence:** There exists a unique dominant eigenvalue Î»â‚ = 1
2. **Positivity:** Corresponding eigenvector r has all positive entries
3. **Dominance:** All other eigenvalues satisfy |Î»áµ¢| < Î»â‚

The damping factor makes M primitive, guaranteeing these properties.

### Method 1: Power Iteration

#### Algorithm

```
Initialize: râ½â°â¾ = [1/n, 1/n, ..., 1/n]áµ€

Iterate: râ½áµâºÂ¹â¾ = M Â· râ½áµâ¾

Stop when: ||râ½áµâºÂ¹â¾ - râ½áµâ¾|| < Îµ
```

#### Why It Works

Any vector can be expressed as:
```
râ½â°â¾ = câ‚vâ‚ + câ‚‚vâ‚‚ + ... + câ‚™vâ‚™
```
where váµ¢ are eigenvectors with eigenvalues Î»áµ¢.

After k iterations:
```
M^k Â· râ½â°â¾ = câ‚Î»â‚áµvâ‚ + câ‚‚Î»â‚‚áµvâ‚‚ + ... + câ‚™Î»â‚™áµvâ‚™
           = câ‚(1)áµvâ‚ + câ‚‚Î»â‚‚áµvâ‚‚ + ... + câ‚™Î»â‚™áµvâ‚™
```

Since Î»â‚ = 1 and |Î»áµ¢| < 1 for i > 1:
```
lim[kâ†’âˆ] M^k Â· râ½â°â¾ = câ‚vâ‚
```

The first term dominates!

#### Convergence Rate

```
||râ½áµâºÂ¹â¾ - r*|| â‰¤ |Î»â‚‚|áµ Â· ||râ½â°â¾ - r*||
```

**Spectral gap:** |Î»â‚‚| determines convergence speed
- Larger gap (|Î»â‚‚| << 1): Fast convergence
- Smaller gap (|Î»â‚‚| â‰ˆ 1): Slow convergence

Typical: |Î»â‚‚| â‰ˆ 0.85 (damping factor), so ~100 iterations needed.

#### Complexity

```
Per iteration: O(nÂ²) matrix-vector multiplication
Total: O(kÂ·nÂ²) where k â‰ˆ 100
```

For sparse networks: O(kÂ·edges) â‰ˆ O(kÂ·n)

### Method 2: Eigendecomposition

#### Algorithm

```
1. Compute eigendecomposition: L = VÎ›Vâ»Â¹
2. Find eigenvalue Î»áµ¢ closest to 1
3. Extract corresponding eigenvector váµ¢
4. Normalize: r = váµ¢ / ||váµ¢||â‚
```

#### Eigenvalue Decomposition

```
L = VÎ›Vâ»Â¹

where:
V = [vâ‚ vâ‚‚ ... vâ‚™]  (eigenvectors as columns)
Î› = diag(Î»â‚, Î»â‚‚, ..., Î»â‚™)  (eigenvalues on diagonal)
```

#### Complexity

```
O(nÂ³) for dense matrices
```

**When to use:**
- Small networks (n < 50)
- Need exact solution
- Research/validation purposes

**Advantages:**
- Exact solution (no iteration)
- Computes all eigenvalues

**Disadvantages:**
- Slow for large networks
- High memory usage
- Unnecessary computation (only need one eigenvector)

### Dangling Nodes

**Problem:** Nodes with no outgoing links create zero columns in L.

**Solution:** Replace zero columns with uniform distribution:
```
if Î£áµ¢ L[i,j] = 0:
    L[:,j] = [1/n, 1/n, ..., 1/n]áµ€
```

### Comparison: Power Iteration vs Eigendecomposition

| Metric | Power Iteration | Eigendecomposition |
|--------|----------------|-------------------|
| Complexity | O(kÂ·nÂ²) | O(nÂ³) |
| Typical k | ~100 | N/A |
| Scalability | Excellent | Poor |
| Accuracy | ~10â»â¶ | Exact |
| Memory | O(nÂ²) | O(nÂ²) |
| Sparse support | Yes (O(kÂ·n)) | Limited |

For n=100: Power iteration is ~7-10Ã— faster
For n=1000: Power iteration is ~100Ã— faster
